let path = require('path');
let schedule = require('node-schedule');
let fs = require('fs');
let request = require('request-promise');
const util = require('util');
const exec = util.promisify(require('child_process').exec);
require('dotenv').config();

username =  process.env.DB_USERNAME;
password = process.env.DB_PASSWORD;
hostname = process.env.DB_HOSTNAME_AWS;
lzip = process.env.DB_LZIP_AWS;
// hostname = process.env.DB_HOSTNAME_AZURE;
// lzip = process.env.DB_LZIP_AZURE;

// setup scheduler to run every 10 minute -- just for testing
let j = schedule.scheduleJob('0 0-23/4 * * *', function(){
// let j = schedule.scheduleJob('0-59/30 * * * * *', function(){

// upload function: upload files to landing zone
let upload = (filename) => {
  let todaysFileName = filename;
  let todaysFilePath = path.join(__dirname, './data/' + todaysFileName);
  return new Promise((resolve, reject) => {
    if (fs.existsSync(todaysFilePath)) {
      let _clusterAddrAndPort = hostname;
      let _ClusterIP = lzip;
      let _mimetype = 'text/csv';
      let _fileStream = fs.createReadStream(todaysFilePath);
      let _clusterFilename = path.basename(todaysFileName);
        request({
          method: 'POST',
          // auth: {'user' : username, 'password' : password},
          uri: _clusterAddrAndPort + '/Filespray/UploadFile.json?upload_' +
            '&NetAddress=' + _ClusterIP + '&rawxml_=1&OS=2&' +
            // 'Path=/var/lib/HPCCSystems/mydropzone/hpccsystems/covid19/file/raw/Scraped/',
            'Path=/var/lib/HPCCSystems/mydropzone/hpccsystems/covid19/file/raw/',
          formData: {
            'UploadedFiles[]': {
              value: _fileStream,
              options: {
                filename: _clusterFilename,
                contentType: _mimetype
              }
            },
          },
          resolveWithFullResponse: true
        }).then((response) => {
          console.log(response.body);
          console.log('Upload Finished\n');
          resolve(response);
        }).catch((err) => {
          console.log(err);
          reject(err);
        })
      }
    }
  )}

// define the scraped files generated by scrape.py
let world_cumconfirmed = 'world_cumulative_confirmed.csv';
let world_newCases = 'world_newCases.csv';
let country_cumdeaths = 'country_cumulative_deaths.csv';
let country_cumconfirmed = 'country_cumulative_confirmed.csv';
let state_cumdeaths = 'states_cumulative_deaths.csv';

let filenames = [world_cumconfirmed,world_newCases,
            country_cumdeaths, country_cumconfirmed,
            state_cumdeaths
          ];

// scrapeAndupload function:
// run the scrape.py to generate the scraped files and upload to LZ
async function scrapeAndupload() {
  const { stdout, stderr } = await exec('python3 ./scrape.py');
  console.log('stdout:', stdout);
  console.error('stderr:', stderr);

  for(i = 0 ; i < filenames.length; i++){
    filename = filenames[i];
    let uploadresult = await upload(filename)
    // upload(filename)
    }
}
     

// run the scrapeAndupload()
scrapeAndupload();


});